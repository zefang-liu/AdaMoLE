--model_path=meta-llama/Llama-2-7b-hf
--data_path=tau/commonsense_qa
--peft_type=adamole
--lora_rank=32
--target_modules
q_proj
k_proj
v_proj
o_proj
--num_experts=8
--threshold=0.125
--max_length=256
--batch_size=4
--gradient_accumulation_steps=4
--num_train_epochs=2
--learning_rate=1e-4
--lr_scheduler_type=constant_with_warmup
--warmup_steps=200
--weight_decay=0.0
--aux_loss_coeff=1e-3